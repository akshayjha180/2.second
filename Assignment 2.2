Explain in detail:
● HDFS
• Hadoop Distributed File System(HDFS) is the primary distributed storage used by Hadoop applications. 
  A HDFS cluster primarily consists of a NameNode that manages the file
  system metadata and DataNodes that store the actual data.
• The file store in HDFS provides scalable, fault-tolerant storage at low cost.
• The HDFS software detects and compensates for hardware issues, including disk problems
  and server failure.
• HDFS stores files across the collection of servers in a cluster.
• Files are decomposed into blocks and each block is written to more than one of the
  servers.
• The replication provides both fault-tolerance and performance.


● Hadoop cluster
• A Hadoop cluster is a special type of computational cluster designed specifically for storing and 
  analyzing huge amounts of unstructured data in a distributed computing environment.
  clusters run Hadoop's open source distributed processing software on low-cost commodity computers.
  Typically one machine in the cluster is designated as the NameNode and another machine the as
• JobTracker- these are the masters. The rest of the machines in the cluster act as both DataNode
• TaskTracker- these are the slaves. Hadoop clusters are often referred to as "shared nothing"
  systems because the only thing that is shared between nodes is the network that connects them. 
• Hadoop clusters are known for boosting the speed of data analysis applications. They also are 
• Highly scalable: If a cluster's processing power is overwhelmed by growing volumes of data, additional 
  cluster nodes can be added to increase throughput. 
• Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other
  cluster nodes, which ensures that the data is not lost if one node fails.


● HDFS blocks
• When you store a file in HDFS, the system breaks it down into a set of individual blocks and stores  
  these blocks in various slave nodes in the Hadoop cluster. This is an entirely normal thing to do, 
  as all file systems break files down into blocks before storing them to disk.
• HDFS only wants to make sure that files are split into evenly sized blocks that match the predefined 
  block size for the Hadoop instance (unless a custom value was entered for the file being stored). 
  In the preceding figure, that block size is 128MB.
• A Hard Disk has concentric circles which form tracks.
• One file can contain many blocks. These blocks in a local file system are nearly 512 bytes and are
  not necessarily continuous.
• For HDFS, since it is designed for large files, the block size is 128 MB by default. Moreover, it gets
  blocks of local file system contiguously to minimise the head seek time.

